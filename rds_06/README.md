## RDS 06: Возьмёте Бэтмобиль?

Построение модели классификации изображений и участие в соотвестующем [соревновании на Kaggle](https://www.kaggle.com/c/sf-dst-car-price-prediction-part2/ "соревновании на Kaggle")

![](https://lms.skillfactory.ru/assets/courseware/v1/67aff42fc790c8a2282dc4b33f19a490/asset-v1:Skillfactory+DST-9+11DEC2019+type@asset+block/20573_Clipboard10_122_1113lo.jpg =400x200)

### Данные для создания модели

Организаторами соревнования предоставлен транировочный датасет, состоящий из 15 561 фотографий, разделенный на 10 классов в соотвествии с моделью автомобиля. Разделение на классы достаточно равномерное.

Работу получившейся модели предлагется проверить на тестовом датасете состоящем из 6 675 неразмеченных на классы фотографий автомобилей.

### Цели и задачи проекта

#### Цели проекта (к чему будем стремиться): 
Построить максимально эффективную нейросетевую модель и показать максимально возможный результат (метрика "accuracy") в соревновании на Kaggle.

#### Задачи проекта (что в идеале хотелось бы сделать):
1. Улучшить результат baseline-решения по метрике "accuracy".
2. Применение transfer learning с finetuning.
3. Попробовать настроить learning rate, optimizer, loss, число эпох.
4. Попробовать подобрать другие переменные (размер картинки, батч и т.д.).
5. Попробовать другие архитектуры сетей и/или их ансамбли (SOTA на ImageNet).
6. Попробовать разные архитектуры «головы» и Batch Normalization.
7. Попробовать применить другие функции callback в Keras.
8. Попробовать применить TTA (Test Time Augmentation).
9. Попробовать применить разные техники управления learning rate.
10. Применить внешние датасеты.
11. Попробовать применить более продвинутые библиотеки аугментации изображений (например, albumentations или imgaug).
12. Обернуть модель в сервис на Flask.

### Итоги и выводы

Сcылка на мой Kaggle Kernel c лучшим резульататом:

https://www.kaggle.com/testdriver87/rds-05-by-petr-skokov-var4?scriptVersionId=43687411

[x] Лучший результат в совреновании на Kaggle в данный момент 0,96779.

[х]  Применен transfer learning с finetuning. На первом этапе в модели заменяем только "голову" модели и обучаем со сравнительно высоким learning rate, на втором этапе "размораживаем" половину слоев базовой модели и дообучаем модель с немного меньшим learning rate, на третьем этапе размораживаем все слои и окончательно дообучаем модель с самым маленьким значением learning rate.

[х] Проведена настройка learning rate и числа эпох; optimizer и loss оставлены без изменений, так как и так были использованы оптимальные значения.

[х] Подобрана настройка батча, чтобы обеспечить компромисс между скоростью и временем создания модели.

[ ] Не хватило времени поэксперементировать с размером картинки. Добавим это в планы по дайльнейшей работе.

[х] Выбрана другая базовая модель нейронной сети - EfficientNet B6. Это дало некоторый прирост.

[ ] Не хватило времени попровать имплементировать ансамблирование. Добавим это в планы по дайльнейшей работе.

[х] Эксперименты с "головой" модели не дали прироста. Batch Normalization уже входит в EfficientNet B6.

[х] Попробовал применить стандартный коллбек ReduceLROnPlateau, который снижает learning rate, при отсуствии прироста метрики в течение ескольких эпох обучения. Это не дало прироста финальной метрики.

[х] Попробовал применить кастомный коллбек CLR (https://github.com/bckenstler/CLR), который внедряет циклическую технику управления learning rate технику Cycle LR для. Это не дало прироста финальной метрики.

[х] Попробовал имплементировать TTA. Метрика значительно ухудшилась. Возможно, нужно больше времени, чтобы поэкспериментировать с данной техникой.

[х] Попробовал применить библиотеку аугментации изображений albumentations. Использование данной библиотеки внесло значительный вклад в прирост финальной метрики.

[ ] Внешние датасеты не примеялись.

[ ] Сервис на Flask не развернут.

Подводя итоги, наибольший вклад в прирост финальной метрики внесли transfer learning с finetuning и использование продвинутой библиотеки аугментации albumentations. Последняя определенно нуждается в дополнительном изучении и настройке, так как при ее помощи можно выиграть еще несколько десятых в финальной метрике.
 
